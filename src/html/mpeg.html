<!DOCTYPE HTML>
<html lang="en">
<head>
	<meta charset="UTF-8">                                              <!-- HTML 5 -->
	<title>MPEG notes</title>
	<!-- META_INSERT -->
	<!-- CSS_INSERT -->
	<!-- JAVASCRIPT_INSERT -->
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']]
			},
			displayAlign: "left",
			displayIndent: "2em"
		});
	</script>
	<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
<div id="header">
	-- This is JEHTech --
</div>

<div id="sidebar">
	<h1 class="title">Links...</h1>
	<div id="includedContent"></div>
</div>

<div id="content">

<!-- START CONTENT -->
<h1 class="title">MPEG</h1>
<div style="padding-right:10px;">
	<h2>Page Contents</h2>
	<div id="page_contents">
	</div>

	<h2>References</h2>
	<div>
		<ol>
			<li>&quot;Digital Video: An Introduction to MPEG-2&quot;, B. G. Haskell et al.
			</li>
			<li>&quot;Telecomminications Handbook&quot; K. Terplan, P. Morreale.
			</li>
			<li><a href="http://downloads.bbc.co.uk/rd/pubs/reports/1996-02.pdf"
				target="_blank">&quot;Research and Development Report. MPEG-2: Overview of the ystems layer&quot;, BBC</a>.
			</li>
			<li><a href="https://www.youtube.com/watch?v=sojvaEauAVo"
				target="_blank">Transport Stream Overview, Ensemble Designs</a>.
			</li>
			<li><a href="https://www.youtube.com/watch?v=t22HpVwybso"
				target="_blank">Packet Timing, Enseble Design</a>.
			</li>
			<li><a href="http://www.tvtechnology.com/expertise/0003/asynchronous-interfaces-for-video-servers/183969"
				target="_blank">&quot;Asynchronous Interfaces For Video Servers&quot;, TVTechnology article</a>.
			</li>
			<li><a href="http://www.silabs.com/support%20documents/technicaldocs/an377.pdf"
			       target="_blank">Timing and Synchronisation In Broadcast Video</a>, Silicon Labs.
			</li>
			<li><a href="http://www.haberdar.org/discrete-cosine-transform-tutorial.htm"
			     target="_blank">Hakan Haberdar, "Generating Random Number from Image of a Probability Density Function"</a>,
				Computer Science Tutorials [online], (Accessed 09-26-2016).
			</li>
			<li><a href="https://www.vcodex.com/historical-timeline-of-video-coding-standards-and-formats/"
				target="_blank">Historical Timeline of Viedo Coding Standards</a>, VCodex.com.
			</li>
			<li><a href="https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/beginnzi.htm"
				target="_blank">A Biginners Guide for MPEG-2 Standard</a>, Victor Lo, City University of Honk Kong.
			</li>
			<li><a href="http://info.tek.com/www-a-guide-to-mpeg-fundamentals-and-protocol-analysis.html"
				target="_blank">A Guide to MPEG Fundamentals and Protocol Analysis</a>, Tektonix.
			</li>
			<li><a href="http://www.streaminglearningcenter.com/AME_Reference.pdf"
				target="_blank">Mastering the Adobe Media Encoder: Critical Concepts and Definitions</a>,
				Jan Ozer -- This is a really good high level tutorial about bit rates and frame rates, resolution,
				CBR and VBR and how the inter-relate in terms of the final encoded video quality.
			</li>
			<li>TOREAD: <a href="http://dranger.com/ffmpeg/tutorial01.html" target="_blank">An ffmpeg and SDL Tutorial</a>, dranger.com
			</li>
			<li>How Video Works, M. Weise &amp; D. Weynand. ISBN 0-240-80614-X
			</li>
            <li><a href="https://ffmpeg.org/ffmpeg-filters.html" target="_blank">FFMPEG Filters</a>.
            </li>
            <li><a href="https://trac.ffmpeg.org/wiki/Scaling%20(resizing)%20with%20ffmpeg" target="_blank">Scaling (Resizing) With FFMPEG.</a>.
            </li>
            <li><a href="https://www.youtube.com/watch?v=1NI7-tRYsPo" target="_blank">What Is DVB-ASI?</a>, Ensemble Designs (YouTube video).
            </li>
            <li>
       			<a href="http://www.rane.com/note145.html" target="_blank">Audio Specifications</a>, Rane Audio Products. [Great little summary of things like THD+N].
            </li>

		</ol>
	</div> <!-- END H2: References -->

    <h2>To Do / To Read</h2>
    <div>
        <ul>
            <li>https://sidbala.com/h-264-is-magic/</li>
            <li>
            	<a href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=12&ved=2ahUKEwjssq-m6f3dAhWryIUKHSapA24QFjALegQIBxAC&url=https%3A%2F%2Fwww.springer.com%2Fcda%2Fcontent%2Fdocument%2Fcda_downloaddocument%2F9781461422297-c1.pdf%3FSGWID%3D0-0-45-1338306-p174267072&usg=AOvVaw0xMRH5J9Wv8PP8d4eRprJq" target="_blank">Chapter 2: Understanding the Application: An Overviewof the H.264 Standard</a>, Scalable Parallel Programming Applied To H.264/AVC.
            </li>
            <li>http://forum.blu-ray.com/showthread.php?t=121087</li>
            <li>https://en.wikipedia.org/wiki/Dialnorm</li>
            <li>http://www.theonlineengineer.org/DownLoadDocs/Meas%20Dialnorm%20Aeq.pdf</li>
            <li>https://en.wikipedia.org/wiki/Dynamic_range</li>
            <li>https://en.wikipedia.org/wiki/DBFS</li>
            <li>http://www.dolby.com/uploadedFiles/Assets/US/Doc/Professional/38_LFE.pdf</li>
            <li>http://www.dolby.com/us/en/technologies/all-about-audio-metadata.pdf</li>
            <li>http://www.dolby.com/us/en/technologies/dolby-digital.pdf</li>
            <li>http://www.dolby.com/uploadedFiles/Assets/US/Doc/Professional/38_LFE.pdf</li>
            <li>http://www.tvtechnology.com/miscellaneous/0008/a-closer-look-at-audio-metadata/184230</li>
            <li>http://www.tvtechnology.com/audio/0014/what-is-downmixing-part-1-stereo-loro/184912</li>
            <li>http://www.dolby.com/us/en/technologies/a-guide-to-dolby-metadata.pdf</li>
            <li>http://www.dolby.com/us/en/technologies/all-about-audio-metadata.pdf</li>
            <li>http://www.tvtechnology.com/miscellaneous/0008/a-closer-look-at-audio-metadata/184230</li>
            <li>http://www.digitaltrends.com/home-theater/ultimate-surround-sound-guide-different-formats-explained/</li>
            <li>http://www.popenmedia.nl/themas/Readers/Geluid/209_Dolby_Surround_Pro_Logic_II_Decoder_Principles_of_Operation.pdf</li>
            <li>http://educypedia.karadimov.info/library/208_Dolby_Surround_Pro_Logic_Decoder.pdf</li>
            <li>http://www.acoustics.salford.ac.uk/placement/chaffey.pdf</li>
            <li>http://www.soundandvision.com/content/surround-decoding-101#zxxiKtKwQVJ6KEDC.97</li>
            <li>https://en.wikipedia.org/wiki/Auditory_masking</li>
            <li>http://legeneraliste.perso.sfr.fr/?p=echelle_log_eng <em>-- log scales - proprtionality</em></li>
            <li>https://www.safaribooksonline.com/library/view/handbook-for-sound/9780240809694/010_9781136122538_chapter2.html#sec2.2</li>
            <li>Excellent huffman coding tutorial: https://www.youtube.com/watch?v=ZdooBTdW5bM</li>
            <li>https://www.youtube.com/watch?v=5wRPin4oxCo</li>
            <li>Run length coding: https://www.youtube.com/watch?v=ypdNscvym_E</li>
            <li>https://www.youtube.com/watch?v=rC16fhvXZOo</li>
            <li>https://www.researchgate.net/publication/220931731_Image_quality_metrics_PSNR_vs_SSIM</li>
            <li>http://repositorio.ucp.pt/bitstream/10400.14/4220/1/lteixeira.2008.conference_ICCCN_IMAP08.pdf</li>
            <li>https://www.gearslutz.com/board/post-production-forum/184636-definitive-explanation-29-97-23-98-timecode.html</li>
            <li>http://wolfcrow.com/blog/understanding-terminology-progressive-frames-interlaced-frames-and-the-field-rate/</li>
            <li>https://www.gearslutz.com/board/post-production-forum/184636-definitive-explanation-29-97-23-98-timecode.html</li>
            <li>http://www.paradiso-design.net/videostandards_en.html</li>
            <li>http://datagenetics.com/blog/november32012/index.html</li>
            <li>https://codesequoia.wordpress.com/2014/02/24/understanding-scte-35/</li>
            <li>http://pdfserv.maximintegrated.com/en/an/AN734.pdf AND http://www.ni.com/white-paper/4750/en/</li>
            <li>https://www.provideocoalition.com/field_order/</li>
            <li>https://ultrahdforum.org/wp-content/uploads/2016/12/Ultra-HD-Forum-Deployment-Guidelines-v1.2-December-2016.pdf</li>
            <li>Color: http://poynton.ca/notes/colour_and_gamma/GammaFAQ.html#gamma</li>
            <li>Color: https://poynton.ca/PDFs/coloureq.pdf</li>
            <li>Color: https://help.prodicle.com/hc/en-us/articles/115015911188-Color-Space-and-Transfer-Function</li>
            <li>https://www.fortinet.com/content/dam/fortinet/assets/white-papers/wp-ip-surveillance-camera.pdf</li>
        </ul>
    </div> <!-- H2 TODO intro -->

    <h2>Glossary</h2>
    <div>
        <p>
            <table class="jehtable">
                <thead>
                    <tr><td>Acronym</td><td>Meaning</td><td>Brief Description</td>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                    	<td>ATSC</td>
                    	<td>Advanced Television Systems Committee</td>
                    	<td>An international, non-profit organization developing voluntary standards for digital television.</td>
                    </tr>
                    <tr>
                    	<td>NTSC</td>
                    	<td>National Television System Committee</td>
                    	<td>NTSC is the video system or standard used in North America and most of South America.</td>
                    </tr>
                    <tr>
                    	<td>PAL</td>
                    	<td>Phase Alternating Line</td>
                    	<td>A colour encoding system for analogue television used in broadcast television systems.</td>
                    </tr>
                    <tr>
                    	<td>SECAM</td>
                    	<td>Sequentiel couleur a memoire - French for &quot;Sequential colour with memory&quot;</td>
                    	<td></td>
                    </tr>
                    <tr>
                    	<td>SSM</td>
                    	<td>Source Specific Multicast</td>
                    	<td>
                    		<q>A method of delivering multicast packets in which the only packets that are delivered to a receiver are those originating from a specific source address requested by the receiver. By so limiting the source, SSM reduces demands on the network and improves security</q> -- <a href="https://en.wikipedia.org/wiki/Source-specific_multicast" target="_blank">From WikiPedia</a>.
                    	</td>
                    </tr>
                    <tr>
                    	<td>UPID</td>
                    	<td>Unique Program Identifier</td>
                    	<td></td>
                    </tr>
		    		<tr>
		    			<td>VUI</td>
		    			<td>Video Usability Information</td>
		    			<td></td>
		    		</tr>
		    		<tr>
		    			<td>SEI</td>
		    			<td>Supplementa Enhancement Information</td>
		    			<td></td>
		    		</tr>
		    		<tr>
		    			<td>PPS</td>
		    			<td>Picture Parameter Set</td>
		    			<td>Introduced in H.264/AVC in response to the devastating effects of a loss of the sequence header and picture header. <a href="https://www.quora.com/What-are-SPS-and-PPS-in-video-codecs" target="_blank">[Ref]</a></td>
		    		</tr>
		    		<tr>
		    			<td>SPS</td>
		    			<td>Sequence Parameter Set</td>
		    			<td>Introduced in H.264/AVC in response to the devastating effects of a loss of the sequence header and picture header. <a href="https://www.quora.com/What-are-SPS-and-PPS-in-video-codecs" target="_blank">[Ref]</a></td>
		    		</tr>
                </tbody>
            </table>
        </p>
    </div>

	<h2>Video Introduction</h2>
	<div>
		<h3>The Basics</h3>
		<p>
			Video is a series of still images that are displayed quickly enough, one after the other, so that
			the human eye/brain percieves only continuous motion and not individual stills. Each still image is
			called a <b>frame</b>. 
		</p>
		<p>
			First video screens were CRTs. An electron beam was &quot;fired&quot; at a screen which contained
			rows of pixels. A pixel was phosphorous and would illuminate when the electron beam hit it. By
			scanning the electron beam horizontally for each line a picture could be created. The is shown
			in the image below (crudely).
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/electron_beam_scan.jpg" alt="Picture showing an electron beam scanning a CRT display."/>
		</p>
		<p>
			By scanning quickly enough, an image can be &quot;painted&quot; on the screen. Each pixel
			would fluoresce for a certain amount of time so by scanning image after image on the screen
			quickly enough, the human eye/brain perceives a moving picture without any flicker. The eye
			retains an image for somewhere between $\frac{1}{30}$ and $\frac{1}{32}$ seconds. This is
			known as the <b>persistence of vision</b>. This is why the
			the frame rate has to be greater than roughly 32Hz, otherwise flicker will be perceived.
		</p>
		<p>
			Consumer video displays between 50 and 60 frames per second. Computer
			displays typically display between 70 and 90 frames per second. The number of full frames per
			second is called the <b>frame rate</b>. It is sometimes refered to as the <b>temperal resolution</b> of
			the image: how quickly the image is being scanned.
		</p>

		<h3>The Early Standards</h3>
		<p>
			Very early TV standards were developed by the NTSC (named after National Television System Committee) in the Americas
			and in Europe a system call PAL was used. France and Soviets used SECAM.
		</p>
		<p>
			NTSC uses 30 fps and 525 lines per frame. 480 lines actively used. 4:3 aspect ratio.
		</p>
		<p>
			PAL &amp; SECAM use 25 fps and 625 lines per frame. 580 lines actively used. 4:3 aspect ratio (orig) or 16:9 aspect ratio (later).
		</p>
		<p>
			Not all of the lines are actively used due to vertical blanking. Horiztonal
			blanking occurs before and after each line. As said, the blanking information
			is part of the early video signals:
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/blanked_and_active_video.jpg" alt="Picture showing blanking regions on a video display"/>
		</p>

		<h3>Interlaced v.s. Progressive</h3>
		<p>
			But wait, haven't we just said that persistence of vision lasts between $\frac{1}{30}$ and $\frac{1}{32}$ seconds?
			Yes, and that is a problem. At 30 fps for NTSC and 25 fps for PAL/SECAM, it is possible that visual
			flicker could be seen. The frame rate needs to be increased. This is why  interlaced video was invented.
			By transmitting the odd lines and then the even lines, for the same amount of data, the frame rate can be doubled,
			which helps eliminate the effects of flicker. A frame is split up into the odd lines and the event lines, each set
			of lines being transmitted and displayed seperately. Each set of lines is called a <b>field</b>. This process of
			field-by-field scanning is called <b>interlaced scanning</b>. The higher frame rate gets traded off against
			slightly worse vertical flicker for fast moving objects.
		</p>
		<p>
			Modern compression and data rates mean that interlaced scanning isn't needed as much so most displays use
			progressive scans where the frame is just the frame - both odd and even lines. The display fills the
			image line after line. This is prefereable because it is more robust to vertical flicker.
		</p>
		<p>
			This is why we see the &quot;i&quot; and &quot;p&quot; to resolutions. For example, for 720x480i, the
			suffix &quot;i&quot; means interlaced and for 720x480p, suffix &quot;p&quot; means progressive.
		</p>

		<h3>Blanking &amp; Ancillary Data</h3>
		<p>
			References:
		</p>
		<ul>
			<li><a href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=4&ved=2ahUKEwi71uz4oeXeAhXlAcAKHbY6APgQFjADegQIBxAC&url=https%3A%2F%2Fwww.smpte.org%2Fsites%2Fdefault%2Ffiles%2Fsection-files%2F2014_July_Closed_Captioning.pptx&usg=AOvVaw2FB0nautCT7spfMWdx1A9P" target="_blank">Understanding &amp; Troubleshooting  Closed Captions</a>, Tektronix.</li>
		</ul>
		<p>
			When the electron beam has finished one line, it must return to the start of the next
			line. Whilst it is doing this it must be turned off, otherwise it would re-paint
			pixels it shouldn't on its return path. This turning-off-during-return is called
			<b>blanking</b>. There are two forms of blanking: 
		</p>
		<ol>
			<li>horizontal blanking, and</li>
			<li>vertical blanking.</li>
		</ol>
		<p>
			Horizontal blanking is used to disable the electron beam
			as it travels from the end of one line down to the start of the next line. Vertical
			blanking is used when an entire frame has been painted and the beam has to move
			back to the top left of the first scan line.
		</p>
		<p> 
			Blanking information is part of the video signal. This means that there is &quot;blank&quot; data transmitted
			during the blanking interval which not displayed. This is done to give the electron beam enough time
			to be turned off before it returns to the top of the screen.
		</p>
		<p>
			Vertical sync, VSYNC, is timing info indicating when new image starts.
			Horizontal sync, HSYNC, is timing info indicating when new scan line starts.
		</p>
		<p>
			None of the data sent during the blanking intervals is displayed. This means the data can be used to
			carry other information, called either <b>ancillary data</b> (ANC) or blanking information. The vertical blanking
			signal data is refered to as VBI (Vertical Blanking Information). The horizonal blanking information
			is refered to as HBI (Horizontal Blanking Information) .
		</p>
		<p>
			VBI/HBI was part of the analogue signal standards. More modern digital signals have kept the blanking information. The corresponding
			terms are VANC (Vertical Ancillary (Data)) and HANC (Horizontal Ancillary (Data)).
		</p>
		<p> 
			For example, <a href="#closed_captions">closed caption</a> data can be sent as
			part of the VBI or VANC data. Other <a href="https://en.wikipedia.org/wiki/Ancillary_data" target="_blank">ancillary data</a> is sent is part of the horizontal blanking.
		</p>
		<blockquote>
			<p>Ancillary (ANC) data is a means of embedding non-video information, such as audio and metadata, in a serial digital transport stream. Ancillary data packets are located in horizontal (HANC) or vertical (VANC) blanking of a video signal. HANC is used to embed uncompressed audio data in the SDI or HD-SDI stream. VANC is used to embed low-bandwidth data -- information updated on a per-field or per-frame basis. Closed caption data (CCD) and Active Format Description (AFD) are examples of metadata stored as VANC. SMPTE 291m describes the details of Ancillary data packet structures.</p>
			<footer>-- <a href="" target="_blank">Using VANC Insertion with FlipFactory</a>, FlipFactory App Note.</footer>
		</blockquote>
		<p>
			Ancillary data packets include following fields (non-exhaustive!):
		</p>
		<ul>
			<li>DID - Data IDentifier - Gives type of ancillary data. If no SDID gives 1 word data identification, otherwise 2 word data identification.</li>
			<li>SDID - Secondary Data IDentifier - Only valid with DID to give a two word data identification.
		</ul>
		<p>
			Generally, it seems that the upper VANC and &quot;front&quot; HANC regions are used to encode ANC data. This is why sometimes you will see diagrams such as the following <a href="http://www.sbe24.org/wba-sbe-shows/archives/Clinic2009/Conrad-Harris-2009.pdf" target="_blank">[Ref]</a>. It only shows the upper VBI and left HBI. I initially found this confusing because I thought it implied the onther blanking areas didn't exist - they do as was cleared up when I asked about this on SO <a href="https://video.stackexchange.com/a/25276/23697" target="_blank">[Ref]</a>.
		</p>
		<p>
			<img src="##IMG_DIR##/vbi_hbi_ntsc.png"/>
		</p>

		<h3>Digital Standards</h3>
		<p>
			The ATSC standard, named after the Advanced Television Systems Committee, is for <em>digital</em> broadcast. I find the &quot;A&quot;
			really annoying because it makes me think analogue. It isn't! This is a <em>digital</em> standard. It is, like NTSC, an American standard, mostly used in North America.
		</p>
		<p>
			In the same way that we had the NTSC and PAL standards, the European equivalent of ATSC is DVB-T, which stands for Digital Video Boardcasting - Terrestrial.
		</p>
		<p>
			The image below shows where the different digital standards are used (ref: EnEdC [Public domain], via Wikimedia Commons).
		</p>
		<p>
<a title="EnEdC [Public domain], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Digital_broadcast_standards.svg"><img width="512" alt="Digital broadcast standards" src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Digital_broadcast_standards.svg/512px-Digital_broadcast_standards.svg.png"></a>
		</p>

		<h3>SD v.s. HD v.s. 4K v.s. Ultra HD</h3>
		<p>
			SD stands for <u>S</u>tandard <u>D</u>efinition. SD gives us the level of detail of an analogue TV, as
			described above. In Europe we got 576i lines and in America 480i lines giving resolutions of 720×576 and 720×480
			respectively. You may want to read the section 'Aspect Ratios v.s. Resolutions' if you're wondering how the
			different aspect ratios are giving us a 720 width!
		</p>
		<p>
			HD stands for <u>H</u>igh <u>D</u>efinition. It's essentially SD with more pixels. HD comes as
			720p, 1080i, and 1080p. Most modern TVs use progressive scanning as it has less flicker problems.
		</p>
		<p>
			720p = 1280 x 720. 1080p = 1920 x 1080
		</p>
		<p>
			4K is Ultra HD = 3840 x 2160p.
		</p>
		<p>
		<img src="##IMG_DIR##/video_resolutions.png" alt="Video standards resolutions mind map"/>
		</p>

		<h3>HEVC</h3>
		<p>
			HEVC stands for <b>H</b>igh <b>E</b>fficiency <b>V</b>ideo <b>C</b>oding. It is the successor to H.264. It is a compression standard.
		</p>
		<P>
			Supplemental enhancement information (SEI) and video usability information (VUI) carry extra information.
		</P>

		<h3>HDR</h3>
		<p>
			HDR stands for <b>H</b>igh <b>D</b>ynamic <b>R</b>ange. Provides better contrast and color accuracy (expands the range). Not a resolution thing. uses <b>W</b>ide <b>C</b>olour <b>G</b>ammut.
		</p>

		<H3>Misc comparison</H3>
		<p>
			https://wolfcrow.com/blog/understanding-mpeg-2-mpeg-4-h-264-avchd-and-h-265/
		</p>
		<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/XvoW-bwIeyY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		<p></p>
		<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/sisvOeZItb0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		<p></p>
		<h3>S-Log</h3>
		<p>
			Tone repoduction scheme for the high-lights and low-lights. Allows good expose in bright as well as dark parts of image, giving better detail and contrast.
		</p>
		<p>
			<q>The exposure values with which we measure light are not linear</q> <a href="https://www.bhphotovideo.com/explora/video/tips-and-solutions/understanding-log-format-recording" target="_blank">[Ref]</a>. But if we record the voltage of a digital camera's photo dector linearly we will asign more bits to some "exposure stops" than others. This is where a log scale can be applied to even this out.
		</p>

		<h3>Aspect Ratios v.s. Resolutions</h3>
		<p>
			Oh dear! Lets take NTSC, for example. We've said it has an aspect ratio of 4:3, but if this were the
			case, with 480 lines, we should have 640 pixels per line. Yet we've said the standard gives us a resolution of 720x480.
			What gives? This <a href="https://en.wikipedia.org/wiki/Standard-definition_television" target="_blank">Wikipedia article</a>
			explains it. It appears our signal is transmitted to satisfy the SDI bandwidth and I'm guessing this is where the 720
			width is coming from. The display then decodes this and somehow displays the image in the display's actual aspec ratio.
			For example, NTSC 480i has a resoltuion of 720x480. But with 480 lines and an aspect ration of 4:3 it is displayed
			as 640x480. For SD TV this gives rise to something called the <a href="https://en.wikipedia.org/wiki/Pixel_aspect_ratio" target="_blank">pixel aspect ratio</a>.
		</p>

        <h3>ASI / SDI</h3>
        <p>
        	ASI stands for Asynchonous Serial Interface. It is a compressed form of an A/V signal and is really
        	just an encapuslation method in that it carries data between two points and is often used to
        	encapsulate MPEG transport stream data. It carries data at a constant bit rate of 270 Mbps which
        	results in a payload rate of 213 Mbps <a href="https://www.youtube.com/watch?v=1NI7-tRYsPo" target="_blank">[Ref]</a>.
        	ASI can be used to transmit multiple programs.
        </p>
        <blockquote>
            <p>
                ASI is a unidirectional tranmission link to transmit data between digital video equipment ... a streaming
                data format which often carries an MPEG TS ... typically over 75-ohm coax terminated with BNCs ...
                It is strictly just an interface ... the format for how data is carried ... DVB-ASI is carried at a 270 Mbps line rate ...
                derived from a 27 MHz byte clock multiplied by 10 bits.
            </p>
            <footer>
                -- <a href="http://www.tvtechnology.com/expertise/0003/asynchronous-interfaces-for-video-servers/183969" target="_blank">TVTechnology article</a>.
            </footer>
        </blockquote>
        <p>
            SDI converts a 27MHz parallel stream of a diginal video interface into a 270 Mbps serial stream that can also be
            transmitted over a 75-ohm coaxial cable. It carries an uncompressed form of an A/V signal and can only
            transmit the one program.
        </p>
        <p>
            <a href="https://en.wikipedia.org/wiki/Serial_digital_interface#Data_format"
               target="_blank">SDI on Wikipedia</a>.
        </p>
        <p>
            Audio is transmitted over SDI via the formats ancillary data, which is <q>... provided as a standardized transport for non-video payload within a serial digital signal; it is used for things such as embedded audio, closed captions, timecode, and other sorts of metadata...</q>.
        </p>
        <p>
        	My understanding is that SDI is just a "signal" encapsulation format for transmission over a serial coaxial. Using SDI original (aalogue) production content can be transmitted, but digital standards MPEG-2 streams can also be transmitted.
        </p>

        <a id="closed_captions"></a>
        <h3>Subtitles / Closed Captions</h3>
        <p>
        	References:
        </p>
        <ul>
	        <li><a href="https://www.adobe.com/content/dam/acom/en/devnet/video/pdfs/introduction_to_closed_captions.pdf" target="_blank">Introduction to Closed Captions</a>, Adobe Technical Paper.</li>
	        <li><a href="https://evertz.com/resources/eia_608_708_cc.pdf" target="_blank">EIA-608 and EIA-708 Closed Captioning </a>, Sarkis Abrahamian </li>
	        <li><a href="https://www.3playmedia.com/2015/08/11/whats-the-difference-between-cea-608-line-21-captions-cea-708-captions/" target="_blank">What’s the Difference Between CEA-608 (Line 21) Captions &amp; CEA-708 Captions?</a>, Emily Griffin</li>
	        <li><a href="https://www.scte.org/documents/pdf/Standards/SCTE%2020%202012.pdf" target="_blank">SCTE 20 2012 - Methods For Carriage Of CEA-608 Closed Captions And Non-Real Time Sampled Video</a>.</li>
	        <li><a href="https://www.scte.org/documents/pdf/Standards/ANSISCTE212001R2006.pdf" target="_blank">ANSI/SCTE 21 2001R2006</a>, Standard For Carriage Of NTSC VBI Data In Cable Digital Transport Streams.</li>
	    </ul>
        <p>
        	The terms seem interchangeable as far as I can see, although closed captions might imply better functionaility, like being able to say how the captions should look and where they appear on the screen etc.
        </p>
        <blockquote>
        	<p>
        		Closed Captioning is the process of electronically encoding television speech in such a way that, although it is  invisible  to  the  regular  viewer, a  decoder  in  the  television  set  (or  a special  set  top  box)  can  decode  the spoken word and display it as text in the picture area.
        	</p>
        	<footer> -- <a href="https://evertz.com/resources/eia_608_708_cc.pdf" target="_blank">EIA-608 and EIA-708 Closed Captioning</a>, Sarkis Abrahamian</footer>
        </blockquote>
        <p>
        	In either case, the general idea is that the text is the equivalent of the audio content, and is such synchronised with the video so that it can be read whilst watching. They assume the viewer is completely deaf and so describes not only the speach but also sounds, for example, describes the ambient music, or animal sounds etc in the scene.
        </p>
        <p>
        	Two closed caption standards are CEA-608 and CEA-708 <a href="https://www.3playmedia.com/2015/08/11/whats-the-difference-between-cea-608-line-21-captions-cea-708-captions/" target="_blank">[Ref]</a> (also refered to as EIA-608/708). The former is the older standard for analogue TV and the latter is the newer standard for digital TV.
        </p>
        <blockquote>
        	<p>In NTSC and SMPTE 259M digital system, captions are encoded onto line 21 of the Vertical Blanking Interval (VBI) ... [In ATSC] The HD-SDI closed caption and related data is carried in three separate portions of the HD-SDI bitstream. They are the Picture User Data, the Program Mapping Table (PMT) and the Event Information Table (EIT).
			</p>
			<footer> -- <a href="https://evertz.com/resources/eia_608_708_cc.pdf" target="_blank">EIA-608 and EIA-708 Closed Captioning</a>, Sarkis Abrahamian</footer>
        </blockquote>
        <p>
        	I.e., NTSC (analogue) CC -&gt; line 21 of VBI<br/>
        	Digital CC -&gt; Picture User Data in MPEG-2 stream.
        </p>
        <p>
        	In modern, digital data streams blanking information is no longer needed really but is still used. Closed captions that would have been
        	carried in the VBI for an analogue signal, can be embedded into MPEG-2 bitstreams, for example. Standards like SCTE-20 <a href="https://www.scte.org/documents/pdf/Standards/SCTE%2020%202012.pdf" target="_blank">[Ref]</a> and SCTE-21 <a href="https://www.scte.org/documents/pdf/Standards/SCTE%2020%202012.pdf" target="_blank">[Ref]</a>
        	define "a standard for the carriage of CEA-608 Closed Captions ... in MPEG-2 compliant bitstreams" <a href="https://www.scte.org/documents/pdf/Standards/ANSISCTE212001R2006.pdf" target="_blank">[Ref]</a>.
        </p>
        <p>
        	The 3playmedia article <a href="https://www.3playmedia.com/2015/08/11/whats-the-difference-between-cea-608-line-21-captions-cea-708-captions/" target="_blank">[Ref]</a> gives a great comparison of the two:
        </p>
        <table class="jehtable">
        	<thead style="font-weight: bold;">
        		<tr>
        			<td>CEA-608 (Line 21) Captions</td>
        			<td>CEA-708 Captions</td>
        		</tr>
        	</thead>
        	<tbody>
        		<tr><td>Standard for analog television</td>
        			<td>Standard for digital television</td>
        		</tr>
        		<tr><td>Can be transmitted on analog &amp; digital television</td>
        			<td>Can only be transmitted on digital television</td>
        		</tr>
        		<tr><td>Appearance: Uppercase white text on black box background</td>
        			<td>Appearance: User can control caption appearance. Options include: 8 fonts in 3 sizes, 64 text colors, 64 background colors, background opacity, and edged or dropshadowed text</td>
        		</tr>
        		<tr><td>Supports up to 2 languages at a time</td>
        			<td>Supports multilingual captions</td>
        		</tr>
        		<tr><td>Language options are limited to English, Spanish, French, Portuguese, Italian, German, and Dutch (due to lack of special characters)</td>
        			<td>Supports captions in any language (characters &amp; symbols from every alphabet are supported)</td>
        		</tr>
        		<tr><td>Transmitted on Line 21</td>
        			<td>Embedded in MPEG-2 streams</td>
        		</tr>
        		<tr><td>Caption position is fixed.</td>
        			<td>Caption position can be changed. The FCC requires that captions be repositioned if they obscure important visual information.</td>
        		</tr>
        	</tbody>
        </table>
        <p></p>
        <pre>TODO:
https://www.smpte.org/sites/default/files/section-files/CC%20Complete%20SBE.pdf
https://clearview-communications.com/wp-content/uploads/2017/10/Understanding-the-Video-Signal.pdf
https://www.appliedelectronics.com/documents/Guide%20to%20Standard%20HD%20Digital%20Video%20Measurements.pdf
https://www.intersil.com/content/dam/Intersil/documents/an16/an1695.pdf
https://pdfs.semanticscholar.org/731e/6bb78131ab7ce94e9773790ef656b4d23f2b.pdf
https://www.adobe.com/content/dam/acom/en/devnet/video/pdfs/introduction_to_closed_captions.pdf
https://android.googlesource.com/platform/frameworks/base/+/master/media/java/android/media/Cea708CaptionRenderer.java
https://ecfsapi.fcc.gov/file/6511959011.pdf
https://www.atsc.org/standard/a53-atsc-digital-television-standard/
http://www.atsc.org/wp-content/uploads/2015/03/A53-Part-1-2013.pdf
http://ecee.colorado.edu/~ecen5653/ecen5653/papers/iso13818-1.pdf</pre>
	</div> <!-- End: Video Intro -->

	<h2>MPEG Introduction</h2>
	<div>
		<p>
			Pissing around looking at
			<a href="https://github.com/jameshume/py_mpeg_1" target="_blank">PCRs etc in Python simulation</a>.
		</p>
		<p>
			Some terms:

			<table class="jehtable">
				<thead>
					<tr>
						<td>Acronym</td><td>Stands For</td><td>Meaning</td>
					</tr>
				</thead>
				<tr><td>STC</td>
					<td>System Time Clock</td>
					<td>Time that PU should be decoded and presented to output device. 33-bit. Units of 90kHz.</td>
				</tr>
				<tr><td>SCR</td>
					 <td>System Clock Reference</td>
					 <td>Clock Reference at the PES level. 42-bit. Tells demux what STC should be when each clock reference is received. Units of 27MHz.</td>
				</tr>
				<tr><td>DTS</td>
					 <td>Decoding Time Stamp</td>
					 <td>Type of STC.</td>
				</tr>
				<tr><td>PTS</td>
					 <td>Presentation Time Stamp</td>
					 <td>Type of STC.</td>
				</tr>
				<tr><td>PU</td>
					 <td>Presentation Unit</td>
					 <td></td>
				</tr>
				<tr><td>ES</td>
					 <td>Elementary Stream</td>
					 <td>Compressed data from a single source (e.g., video, audio) plus data for sync, id etc.</td>
				</tr>
				<tr><td>PES</td>
					 <td>Packetised Elementary Stream</td>
					 <td>An ES that has been split up into packets of either variable or constant length.</td>
				</tr>
				<tr><td>PSI</td>
					 <td>Program Specific Information</td>
					 <td></td>
				</tr>
				<tr><td>GOP</td>
					<td>Group Of Pictures</td>
					<td></td>
				</tr>
				<tr><td>T-STD</td>
				    <td>Transport Stream System Target Decoder</td>
				    <td>Conceptual model used to model the decoding process during the construction or verification of Transport Streams</td>
				</tr>
			</table>
		</p>
		<ul>
			<li>All streams are <em>byte</em> streams</li>
			<li>Uses packet multiplexing</li>
		</ul>
		<h3>A 30,000 Foot View</h3>
		<p>
			<img src="##IMG_DIR##/mpeg/enc_dec_highlevel.jpg"/>
		</p>
		<p>
			In the above diagram video and audio are recorded. The equipment outputs raw, uncompressed
			data. The video data will be a series of frames and the audio some kind of stream. Whatever
			the format that the equipment outputs in, it is the encoder's job to convert this into
			digitised, <em>compressed</em> data.
		</p>
		<p>
			You'll note that at the encoder end the audio and video data is encoded seperately. The
			encoders run off the same clock so that the audio and video encoding remains in sync.
			The challenge is at the receiver end: the received streams must also be played back in
			sync (e.g., get the lipsync right or display the correct captions at the right time etc.).
			Because there is nothing necessarily tying the transmitter and receiver clocks together,
			and because the transmission media may not result in a constant delay applied to each packet,
			MPEG defines a way to transmit the clock in the <b>transport stream</b> (TS).
		</p>
		<p>
			As said, it is the encoder's job to convert this into
			digitised, <em>compressed</em> data. The reason for this is that the frame rate and resolution
			required generally means that the data rate coming out of
			the video equipment, even after digitising, would be far too great to transmit. The bandwidth
			of most transmission media (e.g., satellite, over the internet etc.) would not be able to
			support these data rates.
		</p>
		<p>
			Enter MPEG(-2). It is the job of the encoder to compress the video and audio data and produce
			an <b>elementary stream</b> which is just a byte stream containing the encoded data. MPEG doesn't
			actually specifiy what the bytes in the video/audio data means.... that's completely up to the encoder, which
			is why you will have noticed if that different MPEG files can require difference codecs (codec stands
			for COder/DECoder). All MPEG does is to define the structure of the stream: i.e., the headers and ordering
            of frame information etc. This stream may be emitted at a constant or variable bit rate (CBR, VBR respectively).
		</p>
		<p>
			Once the <b>elemtary stream</b> (ES) is created the packetiser will chunk it up into packets of either constant
			or variable size. Each packet has a header and payload. The packetised streams are then multiplexed into
			either a <b>program stream</b> (PS) or a <b>transport stream</b> (TS). <em>I am generally ignoring the
			program stream in favour of the transport streams in this discussion</em>.
		</p>
		<p>
			The PS is used in environments that are not noisy. The PS can only support one program. It generally
			consists of long, variable-length packets. Each packet has a header and a corrupt header can mean
			an entire packet loss.
		</p>
		<p>
			The TS, which consists of smaller 188-byte packets, is used in noisy environments as the
			smaller packet size is more ameanable to error recovery and if one packet is corrupted
			it is not a great a loss as for a larger PS packet. Note that MPEG does not specify any error
			recovery mechanism: this must be added by the encapsulating protocol if desired. The TS can
			contain many programs.
		</p>
		<p>
			A summary of the flow from ES to PES to TS is shown below.
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/flow_of_stream_packet_types.jpg"/>
		</p>
		<p>
			Note how the diagram shows that
			a TS packet can only contain data from one PES packet, hence the use of some padding at
			the end of the PES packet didn't fit into an integral number of TS packets - shown by hatched area.
		</p>

		<h3>A Receiver At 30,000 Feet</h3>
		<h4>Intro</h4>
		<p>
			As was mentioned, the receiver must play back all the streams synchronously to ensure that things
			like lipsync are correct. How can it do this?
		</p>
		<p>
			The answer lies in the <b>Program Clock Reference</b> (PCR), the <b>Decoding Time Stamp</b> (DTS)
			and <b>Presentation Time Stamp</b> (PTS) data that is transmitted with the presentation
			data (video, audio, subtitles etc). The PCR allows the receiver to estimate the frequency of
			the encoder clock. The DTS tells the receiver when a <b>Presentation Unit</b> (PU) should be
			decoded and the PTS tells the receiver when a decoded PU should be presented to the display
			device.
		</p>
		<p>
			Shown below, we see that the PTS/DTS are used by the decoders and the PCR is used by
			the system to control the clock that is driving the decoders so that the system runs at the
			same frequency and time as the encoder that transmitted this information.
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/receiver_more_detail_1.jpg"/>
		</p>

		<p>
			Why do we need to stay in sync? Why should the receiver bother about the frequency
			of the encoder? Surely if they both have a local clock set to the same frequency there
			should be no problem. The trouble is that even though two clocks may have the same
			<em>nominal</em> frequency, their <em>actual</em> or <em>real</em> frequencies will
			differ by some small amount. Furthermore, these frequencies will not be constant. They
			will vary with environmental effects such as temperature and change as the crytal
			ages. They will also exhibit a certain amount of jitter. But, what effect can this
			have and why should we be concerned? Let's once again look at the overall process...
		</p>

		<p>
			<img src="##IMG_DIR##/mpeg/enc_dec_with_cbr_vbr.jpg"/>
		</p>

		<p>
			The image above shows that the video equipment is generating X frames per
			second. Each frame is digitised and then encoded. Each frame will take a slightly
			longer or shorter time to encode and result in a larger or smaller representation,
			depending on things like the amount of change from the previous frame, the contents etc.
			Thus, in general the encoding will result in a variable bit rate. This can be converted
			to a constant bit rate by simply adding stuffing bytes to each encoded frame so that the
			bit rate becomes constant.
		</p>
        <p>
            When we talk about bit rate, we mean the size of the video (or audio) data per second. I.e., in
            one second how many bits of data do we expect out of the encoder. For example, if the encoder is
            spewing out data at 500 kbps we know that one second of video data requires 500 kbits
            of data to transmit it. The frame rate and resolution of the video and the bit rate output from the encoder are
            related by the amount of compression that the encoder is able to perform: the frame rate cannot
            be changed, so what determines the bit rate is how much the encoder can compress the frames.
        </p>
		<p>
			Lets consider CBR as it will make things easier (bit rate is constant).
            Now lets assume that the transmit-across-the-network
			stage and the decode stage are all instantaneous. In this case we just decode each frame
			as we get it and just display it... no problem.
		</p>
		<p>
			Now lets imagine that the decode is no longer instantaneous. Like the encoder, it may take
			a little longer to decode some frames and a little less time to decode others. When it takes
			a little longer, what happens to the frame following right behind it? Clearly we need to
			be able to queue up that frame rather than dropping it. And what happens if the decoder
			was really quick on one frame? If the next frame isn't immediately available then the
			decoder either has to output nothing (so we see a video glitch) or hold on (repeat a frame). Thus is might
			be an idea to have a few frames in the buffer ready to decode! The same applies to the encoder: some frames might
            take longer to encode than others, so the decoder also has to decouple from this using a buffer.
            Incidently the variable encoding time is another reason why PTS timestamps are important.
		</p>
		<p>
			Then there is the transmission path. That will often have a delay and when going over Ethernet, for
			example, that delay will vary in a random fashion. So we need a receive buffer to smooth
			this out too.
		</p>
		<p>
			Then there is our local clock v.s. the clock of the encoder. If our clock is faster than
			the encoders, even only by a little bit, we will, over time cause a buffer underrun in the
			decoder. And if we were even only a little slower, eventually we would have to drop
			a frame. That's why keeping in sync with the encoder clock and using the DTS/PTS timestamps
			is important.
		</p>

		<h4>Time Sync</h4>
		<p>
			Using the PCR timestamps, the receiver can discipline its local oscillator so that the local
			clock should be, within some tolerance, synchonised (i.e., same frequency and phase)
			with the encoder's clock. Then once the receiver's clock is correct, the decoder blocks can
			use the PTS and DTS timestamps correctly to play out video/audio/etc synchronously.
		</p>

		<p>
			<b>PCR is measured as 33 bits of 90kHz clock and remaining 9 bits of a 27MHz clock</b>, the
			<b>system clock frequency</b>. The system clock frequency's accuracy is restricted
			by the the MPEG 2 standard:

			$$
			 27 000 000 - 810 Hz \le \text{system clock freq} \le 27 000 000 + 810 Hz
			$$
		</p>
		<p>
			The PCR timestamp is split up into a 9 bit field, the extension, and a
			33 bit field, the base, as follows:

			$$
			   \text{base} = \frac{\text{system clock freq} \times t(i)}{300} \mod 2^{33}
			$$

			$$
			   \text{ext} = \text{system clock freq} \times t(i) \mod 300
			$$

			The quantity $\text{system clock freq} \times t(i)$ just gives the number of
			clock ticks that have occurred at time $t(i)$ seconds.
		</p>
		<p>
			For every 300 clock ticks the $\text{base}$ will increment by 1, whilst the
			$\text{ext}$ increments by 1 for every single clock tick but only ranges
			from 0 to 299.
		</p>
		<p>
			So, $\text{ext}$ is like the high precision portion of the clock time stamp. It
			will go from 0 to 299, and when it wraps back round to 0, $\text{base}$ increments
			by 1. Thus the PCR is defined as follows:

			$$
			   PCR(i) = PCR_{base}(i) \times 300 + PCR_{ext}(i)
			$$

			Where $PCR(i)$ is the number of system clock ticks that have passed at time $t(i)$.
		</p>
		<p>
			The 27MHz portion of the clock only needs 9 bits to represent its maximum value of 299.
			So, last question... Why 33 bits for the 90 kHz clock? I'm only guessing but it could
			be that they wanted to be able to represent (over) a days worth of elaphsed time in
			90kHz clock ticks.
		</p>

		<p>
			Okay, so how do we use these PCR timestamps to acheive at least synchonisation?
		</p>
		<p>
			If we had a perfect system the clocks at both ends would have zero
			drift and zero jitter. The communication channel would have a constant, zero delay and never
			drops or corrupts TS packets.
			In this case all we are doing is making sure the receiver's clock is running at the same
			speed as the encoder's clock. It is easy to do. Could we now just take the difference between
			consequtive PCR timestamps? That would certainly give us the frequency of the encoder's clock
			as we can assume the gap between each PCR at the encoder was constant. However, this tells us
			nothing about the receiver clock w.r.t to the encoder clock. The receiver clock could still
			have a frequency offset. Thus, we need to know when, from the receiver's point of view, each PCR
			timestamp arrives:
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/PCR_clock_timestamping_for_frequency_estimation.jpg"/>
		</p>
		<p>
			To continue with the example let's imaging that the decoder clock is running about 1.5 times
			fater than the encoder clock. Thus we might, from the decoder's point of view, receive the
			following information if we knew that both clocks had a nominal frequency of 10Hz and a PCR
			is transmitted every second:
		</p>
		<table class="jehtable">
			<thead>
				<tr>
					<td>Decoder counts when PCR arrived</td> <td>PCR count received</td> <td>Difference</td>
				</tr>
			</thead>
			<tr>
				<td>0</td> <td>0</td> <td>0</td>
			</tr><tr>
				<td>15</td> <td>10</td> <td>5</td>
			</tr><tr>
				<td>30</td> <td>20</td> <td>10</td>
			</tr><tr>
				<td>45</td> <td>30</td> <td>15</td>
			</tr>
		</table>
		<p>
			The gradient of the time stamp differences is the frequency offset.  Each difference should
			occur once every 10 counts, by the assumptions we've made, so we know that the gradient
			difference is 0.5. Therefore the decoder can figure out it is 1.5 times faster that the
			encoder and correct for this. To begin with, if the frequency offset was very large the decoder
			might &quot;jump&quot; it's clock frequency to be in the same &quot;ball park&quot; as the
			encoder, and from there start to align, probably using some kind of control loop, usually
			PI (Proportional Integral) control.
		</p>
		<p>
			That didn't seem so bad, but unfortunately we don't live in a perfect world. In reality
			both the encoder and decoder clocks will have a certain frequency drift from their nominal
			frequencies and will also suffer from jitter. When the encoder clock wanders, the decoder should
			follow, but in the reverse situation, where the decoder wanders but the encoder does not, the
			decoder should detect this and correct it's wandering quickly. For both, the jitter should be
			filtered out and ignored as much as possible. The third variable is the transmission media. It
			may have a delay, and most probably this delay is not constant - another source of jitter! It may
			also drop or corrupt packets so we might not receive all of the timestamps and some may be received
			out of order and the decoder unit as a whole has to be able to cope with this.
		</p>
		<p>
			Let's not get so complicated so quickly. Now lets imagine a slightly less perfect system.
			The encoder and decoder clocks are still perfect, but the transmission media has a constant
			delay. Let's imagine it is 2 clock ticks.
		</p>
		<table class="jehtable">
			<thead>
				<tr>
					<td>Decoder counts when PCR arrived</td> <td>PCR count received</td> <td>Difference</td>
				</tr>
			</thead>
			<tr>
				<td>0 + 2 = 2</td> <td>0</td> <td>2</td>
			</tr><tr>
				<td>15 + 2 = 17</td> <td>10</td> <td>7</td>
			</tr><tr>
				<td>30 + 2 = 32</td> <td>20</td> <td>12</td>
			</tr><tr>
				<td>45 + 2 = 47</td> <td>30</td> <td>17</td>
			</tr>
		</table>
		<p>
			As we can see the rate of change in the differences is still the same. Thus we'll get the
			frequency right. Once we have the frequency right, we will also be able to pull in the
			phase as well.
		</p>
		<p>
			Now lets make the communications channel zero delay again, but this time with jitter. Lets give it a 4 count jitter. A possible
			scenario is now shown in the table below. The added values now come from jitter and not delay!
		</p>
		<table class="jehtable">
			<thead>
				<tr>
					<td>Decoder counts when PCR arrived</td> <td>PCR count received</td> <td>Difference</td>
				</tr>
			</thead>
			<tr>
				<td>0 + 1 = 1</td> <td>0</td> <td>1</td>
			</tr><tr>
				<td>15 + 4 = 19</td> <td>10</td> <td>9</td>
			</tr><tr>
				<td>30 - 2 = 28</td> <td>20</td> <td>8</td>
			</tr><tr>
				<td>45 + 2 = 47</td> <td>30</td> <td>17</td>
			</tr>
		</table>
		<p>
			Now clearly we could not just look at each difference and use it to adjust the decoder clock! We
			need to take this noise out of our system! Look at the effect this has:
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/Effect_of_jitter.jpg"/>
		</p>
		<p>
			So how do we remove this jitter? (And note this jitter could not only just be from the channel but also either clock). Well,
			we basically have to guess in an informed way! A basic low pass filter might do the trick but presumably the real mechanisms
			implemented by manufacturers are far more complex. At least, for now, we have seen what the challenge of the clock recovery
			is and how MPEG tries to help solve it by providing the PCR timestamps and the D/PTS timestamps.
		</p>

		<h4>PTS v.s. DTS</h4>
		<blockquote>
			<p>
				... Some formats, like MPEG, use what they call "B" frames (B stands for "bidirectional"). The two other kinds of frames are called "I" frames and "P" frames ("I" for "intra" and "P" for "predicted"). I frames contain a full image. P frames depend upon previous I and P frames and are like diffs or deltas. B frames are the same as P frames, but depend upon information found in frames that are displayed both before and after them! ...
			</p>
			<p>
				... So let's say we had a movie, and the frames were displayed like: I B B P. Now, we need to know the information in P before we can display either B frame. Because of this, the frames might be stored like this: I P B B. This is why we have a decoding timestamp and a presentation timestamp on each frame. The decoding timestamp tells us when we need to decode something, and the presentation time stamp tells us when we need to display something ... 
			</p>
			<footer>
				-- <a href="http://dranger.com/ffmpeg/tutorial05.html" target="_blank">An ffmpeg and SDL Tutorial, Tutorial 05: Synching Video</a>.
			</footer>
		</blockquote>
		<p>
		</p>
		<blockquote>
			<p>... Presentation time stamps have a resolution of 90kHz, suitable for the presentation synchronization task ... </p>
			<footer>
				-- <a href="https://en.wikipedia.org/wiki/Presentation_timestamp" target="_blank">WikiPedia</a>.
			</footer>
		</blockquote>
		<p>
		</p>

		<h3>A PES Packet</h3>
		<p>
			<img src="##IMG_DIR##/mpeg/PES_Packet_Header.jpg"/>
		</p>

		<h3>A Transport Stream Packet</h3>
		<p>
			A transport stream packet is shown below. The TS is designed for use in noisy, error-prone, environments. It can include
			more than one program and each program can have its own independent time base. The splitting up of the PES into 188 byte
			packets (size can be greater) also adds to the error resistance. All header fields are stored <b>big-endian</b>.
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/TS_188_Byte_packet.jpg"/>
		</p>
		<p>
			A transport stream packet is at a minimum 188 bytes in length. The first 4 bytes are the header and the remaining 184 bytes are
			the payload. A PES packet will be distributed across several TS packet layloads.
		</p>
		<p>
			When a PCR is present the size increases by another 48 bytes, for example, so not all TS packets will be only 188 bytes. In the
			described base, for instance, the packet size would be 236 bytes.
		</p>
		<P> The <b>continuity counter</b> is a 4-bit field incrementing with each Transport Stream packet with the
            same PID, unless the adaptation field control of the packet is '00' or '10', in which case it does not increment.
		</p>
		<p>
			So, for example, one common operation might be to check whether the TS
			packet contains a PCR. The following test would do it (if you wrote it for real
			you'd just make it a one-liner!):
		</p>

		<pre class="prettyprint linenums">bool adaption_field_length_valid = false;
bool adaption_field_contains_pcr = false;
bool adaption_field_present = (raw_ts_packet[3] &amp; 0x20)

if (adaption_field_present) {
   adaption_field_length_valid = (raw_ts_packet[4] > 0);
   if(adaption_field_length_valid)
      adaption_field_contains_pcr = raw_ts_packet[5] &amp; 0x10;
}

if (adaption_field_contains_pcr) {
   // raw_ts_packet[6] to raw_ts_packet[10] and
   // raw_ts_packet[11] &amp; 0xC0 &gt;&gt; 6
}</pre>

		<p>
			One of the most important fields is the PID. PIDs are unique identifiers, which, via the Program Specific Information (PSI)
			tables, the contents of the TS data packet. Some PIDs are reserved:
		</p>
		<table class="jehtable">
			<tr>
				<td>Program Association Table</td> <td>TS-PAT</td> <td>0x0000</td>
			</tr>
			<tr>
				<td>Conditional Access Table</td> <td>TS-CAT</td> <td>0x0001</td>
			</tr>
		</table>
		<p>
			From the root PID (<code>0x0000</code>), known  as the Program Association Table (PAT), we can find the Program Map Table (PMT), which
			gives a list of programs for this TS. From the PMT we can in turn find each ES associated with that program.
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/PMT_PAT.jpg"/>
		</p>
		<p>
			Other PSI tables are also located via the PAT, such as the Conditional Acess Table (CAT) and Network Information Table (NIT).
			Consult the standard for info on these tables... too much to write here.
		</p>
	</div> <!-- END: H2 -->

	<h2>Video Elementary Streams</h2>
	<div>
		<p>
			A video elementary stream (ES) is laid out as a series of sequence headers followed by a series of other optional, variable sized headers and data until the actual picture data.
			The actual header structures and sequences in which the header data must be parsed is quite detailed so I don't think it is worth replicating here: just gotta go through
			the standard which lays down the structure very clearly. Just as an idea, here is a 30,000 foot view of one picture sequence:
		</p>
		<p>
			<img src="##IMG_DIR##/mpeg/video_es_organisation.jpg"/>
		</p>
		<p>
			Meh, okay here are a few of the header formats, but not all of them by an means!
		</p>

		<div style="float:left; margin-right:10px;">
			<table style="padding: 0px; margin: 0px 0px 0px -2px; border:0px;">
				<tr style="background:black; color:white; padding: 0px; margin:0px; border:0px">
					<td colspan="2" style="margin:0px;">Start Codes</td>
				</tr>
			</table>
			<table style="margin: -3px 0px 0px 0px;">
				<tr>
					<td><b>Name</b></td><td><b>Code</b></td>
				</tr>
				<tr><td>picture_start_code</td><td>0x00</td></tr>
				<tr><td>slice_start_code</td><td>0x01 – 0xAF</td></tr>
				<tr><td>user_data_start_code</td><td>0xB2</td></tr>
				<tr><td>sequence_header_code</td><td>0xB3</td></tr>
				<tr><td>sequence_error_code</td><td>0xB4</td></tr>
				<tr><td>extension_start_code</td><td>0xB5</td></tr>
				<tr><td>sequence_end_code</td><td>0xB7</td></tr>
				<tr><td>group_start_code</td><td>0xB8</td></tr>
				<tr><td>system_start_code</td><td>0xB9 – 0xFF</td></tr>
			</table>
		</div>

		<div style="float:left; margin-right:10px;">
			<table style="padding: 0px; margin: 0px 0px 0px -2px; border:0px;">
				<tr style="background:black; color:white; padding: 0px; margin:0px; border:0px">
					<td colspan="2" style="margin:0px;">Sequence Header</td>
				</tr>
			</table>
			<table style="margin: -3px 0px 0px 0px;">
				<tr><td><b>Field Name</b></td><td><b>Bits</b></td></tr>
				<tr><td>sequence_header_code</td><td>32</td></tr>
				<tr><td>horizontal_size_value</td><td>12</td></tr>
				<tr><td>vertical_size_value</td><td>12</td></tr>
				<tr><td>aspect_ratio_information</td><td>4</td></tr>
				<tr><td>frame_rate_control</td><td>4</td></tr>
				<tr><td>bit_rate_value</td><td>18</td></tr>
				<tr><td>marker_bit</td><td>1</td></tr>
				<tr><td>vbv_buffer_size_value</td><td>10</td></tr>
				<tr><td>contrained_parameters_flag</td><td>1</td></tr>
				<tr><td>load_intra_quantiser_matrix</td><td>1</td></tr>
				<tr><td colspan="2">if (load_intra_quantiser_matrix):</td></tr>
				<tr><td style="padding-left:30px;">intra_quantiser_matrix</td><td>8*64</td></tr>
				<tr><td>load_non_intra_quantiser_matrix</td><td>1</td></tr>
				<tr><td colspan="2">if (load_non_intra_quantiser_matrix):</td></tr>
				<tr><td style="padding-left:30px;">load_non_intra_quantiser_matrix</td><td>8*64</td></tr>
				<tr><td colspan="2">next_start_code</td>
				</tr>
			</table>
		</div>

		<div style="float:left; margin-right:10px;">
			<table style="padding: 0px; margin: 0px 0px 0px -2px; border:0px;">
				<tr style="background:black; color:white; padding: 0px; margin:0px; border:0px">
					<td colspan="2" style="margin:0px;">Sequence Extension</td>
				</tr>
			</table>
			<table style="margin: -3px 0px 0px 0px;">
				<tr><td><b>Field Name</b></td><td><b>Bits</b></td></tr>
				<tr><td>extension_start_code</td><td>32</td></tr>
				<tr><td>extension_start_code_identifier</td><td>4</td></tr>
				<tr><td>profile_and_level_indication</td><td>8</td></tr>
				<tr><td>progressive_sequence</td><td>1 </td></tr>
				<tr><td>chroma_format</td><td>2 </td></tr>
				<tr><td>horizontal_size_extension</td><td>2 </td></tr>
				<tr><td>vertical_size_extension</td><td>2 </td></tr>
				<tr><td>bit_rate_extension</td><td>12 </td></tr>
				<tr><td>marker_bit</td><td>1 </td></tr>
				<tr><td>vbv_buffer_size_extension</td><td>8</td></tr>
				<tr><td>low_delay</td><td>1 </td></tr>
				<tr><td>frame_rate_extension_n</td><td>2 </td></tr>
				<tr><td>frame_rate_extension_d</td><td>5 </td></tr>
				<tr><td colspan="2">next_start_code</td></tr>
			</table>
		</div>

		<div style="float:left; margin-right:10px;">
			<table style="padding: 0px; margin: 0px 0px 0px -2px; border:0px;">
				<tr style="background:black; color:white; padding: 0px; margin:0px; border:0px">
					<td colspan="2" style="margin:0px;">GOP Header</td>
				</tr>
			</table>
			<table style="margin: -3px 0px 0px 0px;">
				<tr><td><b>Field Name</b></td><td><b>Bits</b></td></tr>
				<tr><td>group_start_code</td><td>32 </td></tr>
				<tr><td>time_code</td><td>25 </td></tr>
				<tr><td>closed_gop</td><td>1 </td></tr>
				<tr><td>broken_link</td><td>1 </td></tr>
				<tr><td colspan="2">next_start_code</td></tr>
			</table>
		</div>

		<div style="float:left; margin-right:10px;">
			<table style="padding: 0px; margin: 0px 0px 0px -2px; border:0px;">
				<tr style="background:black; color:white; padding: 0px; margin:0px; border:0px">
					<td colspan="2" style="margin:0px;">Picture Header</td>
				</tr>
			</table>
			<table style="margin: -3px 0px 0px 0px;">
				<tr><td><b>Field Name</b></td><td><b>Bits</b></td></tr>
				<tr><td>picture_start_code</td><td>32</td></tr>
				<tr><td>temporal_reference</td><td>10</td></tr>
				<tr><td>picture_coding_type<br/>
						<table style="border:0px; margin-left:30px;">
							<tr><td>I</td><td>=</td><td>0x1</td></tr>
							<tr><td>P</td><td>=</td><td>0x2</td></tr>
							<tr><td>B</td><td>=</td><td>0x3</td></tr>
						</table>
					</td>
					<td>3</td></tr>

				<tr><td>vbv_delay</td><td>16 </td></tr>
				<tr><td>if (picture_coding_type is P or B)</td><td></td></tr>
				<tr><td style="padding-left:30px;">full_pel_forward_vector</td><td>1 </td></tr>
				<tr><td style="padding-left:30px;">forward_f_code</td><td>3 </td></tr>
				<tr><td colspan="2">if (picture_coding_type is B):</td></tr>
				<tr><td style="padding-left:30px;">full_pel_backward_vector</td><td>1 </td></tr>
				<tr><td style="padding-left:30px;">backward_f_code</td><td>3 </td></tr>
				<tr><td colspan="2">while (nextbits() is '1'):</td></tr>
				<tr><td style="padding-left:30px;">extra_bit_picture with value '1'</td><td>1 </td></tr>
				<tr><td style="padding-left:30px;">extra_information_picture</td><td>8 </td></tr>
				<tr><td>extra_bit_picture with value '0'</td><td> 1 </td></tr>
				<tr><td colspan="2">next_start_code</td></tr>
			</table>
		</div>

		<p style="clear:both;">
			One interesting field is the <code>temporal_reference</code> field in the picture header. As will note in a bit, the coded order and display order are different. The
			<code>temporal_reference</code> tells the decoder where in this sequence the frame lies in terms of its <em>display</em> order.
		</p>

		<h3>Frame Types</h3>
        <p>
            There are three types of pictures that use different coding methods:
        </p>
        <ol>
            <li>
            	<b>Intra-coded (I)</b> pictures are coded using information only from themselves.
            	This means that an I frame can be decoded <em>independently</em> of any other frame.
            	They provide a start point from which future predictively decoded frames can be decoded.
            	Each I-frame is divided into 8x8 pixel blocks, each block being placed in a
            	16x16 block called a <b>macroblock</b>.
            </li>
            <li>
            	<b>Predictive-coded (P)</b> use motion compensated prediction from a past reference
            	frame/field: motion vectors for picture blocks.
            	This means that to decode a P frame you need another frame as reference
            	from which you can derive the P frame. You <em>cannot</em> independently decode a P
            	frame. A P frame can be used as the reference for a later P frame (P frames can be
            	the basis for future prediction).
            </li>
            <li>
            	<b>Bidirectionally predictive-coded (B)</b> pictures use motion compensated prediction
            	 from past and/or future reference frames. Like a P frame, a B frame cannot be independently
            	 decoded. B frames are <em>not</em> used as a reference for future prediction.</li>
        </ol>
        <p>
            This relationship is shown below:
        </p>
        <p>
            <img src="##IMG_DIR##/mpeg/mpeg_frame_types.jpg" />
        </p>
        <p>
            In general the amount of compression that each frame gives increases from I to B. Because the I frame is used as the basis from
            which we can decode a P frame - we use the I frame + motion compensation - the I frame has the least compression as the quality
            of the subsequent P and B frames depend on it. B frames offer the most compression.
        </p>
        <p>
            Interestingly, because B frames exist, the sequence of frames output by an encoder will not likely be the order in which
            they are presented to the end viewer. The frames are output in <b>stream order</b>, which is the order in which the decoder
            must decode them.
        </p>
        <p>
            <img src="##IMG_DIR##/mpeg/DTS_PTS_relationship.jpg" style="float:left; margin-right: 5px; margin-bottom: 5px;"/>

            For example, imagine you have the following frame <em>display</em> order: I<sub>1</sub> P<sub>1</sub> B<sub>1</sub> P<sub>2</sub>. In order to encode
            frame B<sub>1</sub>, the encoder needs both frames P<sub>1</sub> and P<sub>2</sub> as B<sub>1</sub> is a predicated frame based on the
            contents of the past frame P<sub>1</sub> and the future frame P<sub>2</sub>. Thus the <em>stream</em>
            order will be I<sub>1</sub> P<sub>1</sub> P<sub>2</sub> B<sub>1</sub>.
        </p>
        <p>
            To look at this in &quot;real life&quot; I've loaded some random TS file into <a href="http://www.pjdaniel.org.uk/mpeg/" target="_blank">MPEG-2 TS packet analyser</a>
            and read out, from the start of the file, the DTS/PTS timestamps for the first few video frames. Put the values into a spreadsheet, shown to the left (or above depending
            on your window size and how contents have re-flowed).
        </p>
        <p>
            I've &quot;normalised&quot; the original timestamps to make them more readable. I've taken the first DTS value and subtracted that from all PTS and DTS values, which I've then divided by 1800.
            You can see that the DTS increments monotonically and linearly because the frames are transmitted in decoded order, not display order. It is for this reason that you can see
            the PTS values move around.
        </p>
        <p>
            The highlighted boxed show how groups of frames are displayed in a different order
            to the order in which they are decoded. You can see they are decoded <em>before</em>
            they are displayed. This implies that the frames after them in the TS strean rely on
            their contents. This implies that the 2 frames following each highlighted box are
            B frames <em>because they rely on a frame in the future</em>. So, we can infer that frames 6; 10; 14; 18 and so on are B frames. The frames 7; 11; 15; 19 could also
            be B frames, but they could also be I frames.
        </p>
        <p>
            So we can distinguish, just from the frame ordering, which frames are B frames. But, we can only do this for a small subset, the rest could be any type. Luckily, each frame has information embedded in it's header that tells us what type it is. This discussion was really just to illustrate the re-ordering :)
        </p>
        <p style="clear:both;">
        </p>
        <p></p>
        <h3>Groups Of Pictures (GPS) &amp; Coded Video Sequences (CVS)</h3>
        <p>
        	The definition of a GOP is easily found on Wikipedia and aplies mostly to H.264. Apparently it is replaced by CVS in H.265 <a href="https://www.researchgate.net/post/What_is_the_difference_between_Group_of_Pictures_GOP_and_video" target="_blank">[Ref]</a>.
        </p>
        <blockquote>
        	<p>A group of pictures, or GOP structure, specifies the order in which intra- and inter-frames are arranged. The GOP is a collection of successive pictures within a coded video stream. Each coded video stream consists of successive GOPs, from which the visible frames are generated. Encountering a new GOP in a compressed video stream means that the decoder doesn't need any previous frames in order to decode the next ones, and allows fast seeking through the video. </p>
        	<footer>-- <a href="https://en.wikipedia.org/wiki/Group_of_pictures" target="_blank">Wikipedia</a></footer>
        </blockquote>
        <p>
        	Another nice definition is:
        </p>
        <blockquote>
        	<p>
        	The GOP is, as its name shows, a group of frames arranged in a specific order (I-frames, B-frames and P-frames in case of the H.264/AVC standard). The coded video stream is actually a succession of GOPs of a specific size (e.g 8 or 12 frames, which is set in the header of the standard).
	        </p><p>
				A GOP starts always with an I-frame (Intra coded frame or reference frame) also called "key frame". The size of the GOP is the distance between two consecutive I-frames (e.g IBBPBBPBBPBBI means that the size of the GOP is 12).
			</p><p>
				To extract GOP from a coded video, you should look at the ffmpeg documentation. One solution could be:
			</p>
			<pre>ffmpeg -i INPUT.mp4 -acodec copy -f segment -vcodec copy -reset_timestamps 1 -map 0 OUTPUT%d.mp4</pre>
			<p>[This] will split your input mp4 video into a series of numbered output files starting each with an I-frame. 
			</p>
			<footer>-- <a href="https://www.researchgate.net/post/What_is_the_difference_between_Group_of_Pictures_GOP_and_video" target="_blank">ResearchGate.net</a>.</footer>
        </blockquote>
        <p>
        	I have some confusion over wheter CVS is new to H.265 as it appears in the book <a href="https://books.google.co.uk/books?id=9NWnBAAAQBAJ&pg=PA102&lpg=PA102&dq=gop+(group+of+pictures)+vs+cvs+(coded+video+sequence)&source=bl&ots=67Zty0IG2f&sig=ACfU3U0C8k3dOvHXy9ldeYAx8RYaXdfM2Q&hl=en&sa=X&ved=2ahUKEwjRnd7I4JfgAhXNRBUIHbcBB5wQ6AEwCHoECAYQAQ#v=onepage&q=gop%20(group%20of%20pictures)%20vs%20cvs%20(coded%20video%20sequence)&f=false" target="_blank">High Efficiency Video Coding"</a>, which is H.264. To quote:
        </p>
        <blockquote>
        	<p>
        		A video of a given number of picutures may be partitioned into one or multiple coded video sequences ... Each CVS can be decoded independently from over coded video sequences that may be contained in the same bitstream ...
        	</p>
        	<p>... The <i>coding structure</i> comprises a consecutive set of pictures in the sequence with a specific coding order and defined dependencies between the included pictures. The full video sequence may be represented by periodic reprition of this coding structure. The set of pictures that is comprised in the coding structure is ofter called a Group of Pictures (GOP)...
        	</p>
        	<footer>-- <a href="https://books.google.co.uk/books?id=9NWnBAAAQBAJ&pg=PA102&lpg=PA102&dq=gop+(group+of+pictures)+vs+cvs+(coded+video+sequence)&source=bl&ots=67Zty0IG2f&sig=ACfU3U0C8k3dOvHXy9ldeYAx8RYaXdfM2Q&hl=en&sa=X&ved=2ahUKEwjRnd7I4JfgAhXNRBUIHbcBB5wQ6AEwCHoECAYQAQ#v=onepage&q=gop%20(group%20of%20pictures)%20vs%20cvs%20(coded%20video%20sequence)&f=false" target="_blank">High Efficiency Video Coding: Coding Tools and Specification</a>,  Mathias Wien.
        	</footer>
        </blockquote>
        <p></p>
        
    </div> <!-- END H2 Video Streams -->

    <h2>SCTE-35: Advertisment insertion opportunities</h2>
    <div>
        <p></p>
        <blockquote>
            <p>
                SCTE-35 is a standard to signal an advertisement (ad) insertion opportunity in a transport stream ...
                Ads are not hard coded into the TV program. Instead, the broadcasters insert the ad to the time slot on the fly. This allows broadcasters to change ads based on the air time, geographic location, or even personal preference ... Analog broadcasting used a special audio tone called DTMF to signal the ad time slot. In digital broadcasting, we use SCTE-35 ...
            </p>
            <p>
                ... SCTE-35 was originally designed for ad insertion. However, it turned out to be also useful for more general segmentation signaling ...
            </p>
            <footer>-- <a href="https://codesequoia.wordpress.com/2014/02/24/understanding-scte-35/" target="_blank">Understanding SCTE-35</a>
            </footer>
        </blockquote>
        <p></p>
    </div>

    <h2 style="clear:both;">The Different Standards: MPEG-2/4, H.264...</h2>
    <div>
    	<p>https://stackoverflow.com/questions/10477430/what-is-the-difference-between-h-264-video-and-mpeg-4-video</p>
    	<p>https://wolfcrow.com/blog/understanding-mpeg-2-mpeg-4-h-264-avchd-and-h-265/</p>
    	<p>https://wolfcrow.com/blog/what-is-a-video-container-or-wrapper/</p>

    </div> 

	<h2 style="clear:both;">Audio</h2>
	<div>
        <h3>Surround Sound, Audio Mixing</h3>
        <p>
            Links to make notes on:
        </p>
        <p>https://documentation.apple.com/en/logicpro/usermanual/index.html#chapter=39%26section=2%26tasks=true
        </p>
        <p>http://www.dolby.com/us/en/technologies/a-guide-to-dolby-metadata.pdf
        </p>
        <p>http://www.dolby.com/us/en/guide/surround-sound-speaker-setup/5-1-setup.html
        </p>
        <p>https://en.wikipedia.org/wiki/Audio_mixing_(recorded_music)#Downmixing
        </p>
        <p>
            The abbreviations used for surround sound channels are as follows
            <a href="https://documentation.apple.com/en/logicpro/usermanual/index.html#chapter=39%26section=2%26tasks=true" target="_blank">[Ref]</a>
            <a href="http://www.google.com/patents/US9014378" target="_blank">[Ref]</a>
            <a href="http://www.dolby.com/us/en/technologies/dolby-surround-7-1-for-theater-tech-paper.pdf" target="_blank">[Ref]</a>:
        </p>
        <table>
            <tr><td>Abbreviation</td><td>Meaning</td></tr>
            <tr><td>L</td> <td>(Front) Left</td></tr>
            <tr><td>Lc</td>  <td>Left Center</td></tr>
            <tr><td>C</td>  <td>Center</td></tr>
            <tr><td>Rc</td>  <td>Right Center</td></tr>
            <tr><td>R</td>  <td>(Front) Right</td></tr>
            <tr><td>Lm</td>  <td>Left Mid</td></tr>
            <tr><td>Rm</td>  <td>Right Mid</td></tr>
            <tr><td>Ls</td>  <td>Left Surround (Rear Left)</td></tr>
            <tr><td>S</td>  <td>Surround (Rear Center)</td></tr>
            <tr><td>Rs</td>  <td>Right Surround (Rear Right)</td></tr>
            <tr><td>LFE</td>  <td>Low Frequency Effects</td></tr>
            <tr><td>Lvh</td> <td>Left Vertical Height</td></tr>
            <tr><td>Rvh</td> <td>Right Vertical Height</td></tr>
            <tr><td>Lrs</td> <td>Left Rear Sound</td></tr>
            <tr><td>Rrs</td> <td>Right Rear Sound</td></tr>
            <tr><td>Bsl</td> <td>Back Surround Left</td></tr>
            <tr><td>Bsr</td> <td>Back Surround Right</td></tr>
        </table>
        <p>
            You will also sometimes get the abbreviation 3/2 surround. This means the configuration
            uses three front channels (L, R, C) and two rear or surround channes (Ls, Rs)
            <a href="http://www.afterdawn.com/glossary/term.cfm/3_2_surround" target="_blank">[Ref]</a>.
        </p>
        <p>
            TODO: Calify the above and associated with 5.1, 7.1 etc etc
        </p>

        <h3>AES Audio</h3>
        <p>
            <a href="https://tech.ebu.ch/docs/tech/tech3250.pdf"
               target="_blank">Specification of the Digital Audio Interface (The AES/EBU interface)</a>, Third Edition, EBU, 2004. -- <b>Warning:</b> the channel status word appears to wrong in this document as they've mis-labelled the bytes and there are one too many bytes in it! Using <a href="https://en.wikipedia.org/wiki/AES3#Channel_status_word" target="_blank">Wikipedia info</a> instead!
        </p>
        <p>
            <a href="https://tech.ebu.ch/docs/other/aes-ebu-eg.pdf"
               target="_blank">Engineering Guidelines: The EBU/AES Digital Audio Interface</a>, J. Emmett, EBU, 1995.
        </p>
        <p>
            In its original design AES was made to carry "two channels of periodically sampled and linearly
            represented audio", i.e., LPCM data. However, AES can carry compressed audio data instead, for
            which an extension to the standard was made.
        </p>
        <p>
            AES is a sequence of blocks, where each block is composed of 192 frames:
            When LPCM audio is used the frequency at which AES frames occur is the same
            as the sampling rate of the PCM audio.
        </p>
        <p>
            Each of these frames has two 32-bit subframes, one for each of the two audio channels. Each subframe is meant to be one LCPM sample.
            It contains at most
            20 or 24 bits of audio data and the rest if the bits form part of other information structures
            that are spread across the 192 subframes. I.e., these structures are created by appending each
            bit from the 192 subframes into one data block.
        </p>
        <p>
            <img src="##IMG_DIR##/mpeg/aes_block_frame_subframe.jpg"/>
        </p>
        <p>
            As said, &quot;other information&quot; structures are spread across the 192 subframes.
            If you take the C bit from the first subframe for channel A as bit 0 of the channel status data, and the C bit
            from the 192<sup>th</sup> subframe for channel A as bit 192 of the channel status data, you construct
            the channel status message. This message carries information associated with the audio signal and is shown below.
        </p>
        <p>
            <img src="##IMG_DIR##/mpeg/aes_build_up_chan_status.jpg"/>
        </p>
        <p>
            As mentioned, AES frames can carry compressed data. When this is the case bit 1 of the channel configuration
            message indicates whether the audio samples represent linear PCM samples or &quot;something else&quot;.
            When this bit is 0, the samples are LPCM, when it is 1, the samples are &quot;something else&quot;. This
            usually means compressed audio data conforming to some standard, for example Dolby audio etc. The SMPTE
            standard 337M-2000 describes how not-PCM-audio-data can be inserted into an AES3 packet stream. One example
            of not-PCM-audio-data is timestamp data used to sync audio with video.
        </p>
        <p>
            When transmitting not-PCM-sample-data there are two modes available.
        </p>
        <ol>
            <li>Frame mode: here the audio bits from a frame's two subchannels are combined to give a 48-bit chunk of data.
            </li>
            <li>Subframe mode: the channels are treated independently.
            </li>
        </ol>

        <h3>Audio Standards</h3>
        <p>
            <a href="http://www.grc.upv.es/docencia/tra/referencias/AudioCoding/Brandenburg_mp3_aac.pdf"
               target="_blank">MP3 and AAC Explained</a>, K. Brandenburg.
        </p>

        <h3>Audio Quality Measures</h3>
        <h4>Total Harmonic Distortion Plus Noise (THD+N)</h4>
        <p>
            Versus frequency and versus level.
        </p>
        <h4>Frequency Response</h4>
        <p>
        </p>
        <h4>Power vs. Time</h4>
        <p>
        </p>
        <h4>Dynamic Range</h4>
        <p>
        </p>
        <h4>Spectrum Average</h4>
        <p>
        </p>
        <h4>Dialog Normalization</h4>
        <p>
        </p>
	</div><!-- END: H2 - Dunno Yet -->


    <h2>FFMPEG</h2>
    <div>
    	<p>
    		See <a href="https://ffmpeg.org/ffmpeg.htm" target="_blank">the docs</a>!
    	</p>
        <h3>ffmpeg</h3>
        <h4>Intro</h4>
        <p>
        	General command is:
        </p>
        <pre>ffmpeg [global_options] {[input_file_options] -i input_url} ... {[output_file_options] output_url} ...</pre>
        <p>This is how options like <code>-f</code> can say that they force either the input or output file type: whether
        	it forces input or output depends whether it appears before the <code>-i</code> or not.</p>
    	<p>
    		Very useful is to use &quot;<code>-hide_banner</code>&quot; in command lines to get
    		rid of version info (quite large) in output. Note the UNDERSCORE (not hyphen). To make it super quiet 
    		use the option <code>-loglevel panic</code> as the first command line argument.
    	</p>
        <h4>Extract Frames</h4>
        <p>
            To extract each frame of a video file as an individual picture run the following. FFMPEG parses the output
            (<code>printf</code>-like) string and understands from it what you are trying to do.
            It will name the frames <code>frame_00001</code>, <code>frame_00002</code> and so on.
        </p>
        <pre>ffmpeg -i &lt;your-input-file&gt; "frame_%05d.png"</pre>
        <p>
            You can use other options to output just specific frames too...
        </p>
        <pre>ffmpeg -i &lt;your-input-file&gt; -ss 00:00:00:00.000 -vframes 1 &lt;name-of-output-file&gt;</pre>
        <p>
            ... where <code>-vframes</code> sets the number of video frames to output and <code>-ss</code> sets the start time offset.
        </p>
        <p>
            Or, lets say you want to output some number of frames from a specific time, you could write....
        </p>
        <pre>ffmpeg -i &lt;your-input-file&gt; -ss 00:01:11.123 -vframes 10 "frame_%02d.png"</pre>
        <p>
            Or, do the same but not for 10 frames, just for 5 seconds...
        </p>
        <pre>ffmpeg -i &lt;your-input-file&gt; -ss 00:01:11.123 -t 5 "frame_%02d.png"</pre>
        <p>
            If you don't want the full frame quality you can also scale the output images
            and even change them to grey scale if space is an issue. Use the switch
            <code>-s widthxheight</code>, e.g., <code>192x168</code>. You can also use the
            <code>-vf scale=w:h</code>. The
            <a href="https://trac.ffmpeg.org/wiki/Scaling%20(resizing)%20with%20ffmpeg" target="_blank"><code>scale</code></a>
            parameter is neat because if
            you want to keep the same aspect ratio you can just set one of either the width
            or height to <code>-1</code>. You can even use refer to the input scale using the
            variables <code>ih</code>, for input height and <code>iw</code>, for input
            width. Eg, <code>-vf scale=iw/4:-1</code>.
        </p>
        <p>
            To grayscale use the switch <code>-pix_fmt gray</code> (use the command
            <code>ffmpeg -pix_fmts</code> to view avilable formats).
        </p>
        <pre>ffmpeg -i &lt;your-input-file&gt; -vf scale=192:-1 -pix_fmt gray "frame_%05d.png"</pre>
        <p>
            To extract a specific audio channel from a video file use the following. FFMPEG will create
            the type of file you request via the extension. Normally use <code>.wav</code>.
        </p>
        <pre>ffmpeg -i &lt;your-input-file&gt; -map 0:2 &lt;name-of-output-file&gt;</pre>
        <p>
        	The <code>-map</code> option is used for manual control of stream selection in each output file.
        </p>
        <p>
            <a href="https://ffmpeg.org/ffmpeg-filters.html#Filtering-Introduction" target="_blank">Filtering</a> is
            done via to <b><code>-vf</code></b> option. For example to output one image every I-frame:
        </p>
        <pre>ffmpeg -i input.flv -vf "<a href="https://ffmpeg.org/ffmpeg-filters.html#select_002c-aselect" target="_blank">select</a>='eq(pict_type,PICT_TYPE_I)'" -vsync vfr thumb%04d.png</pre>

        <h4>Modify Bitrates</h4>
        <p>
        	One thing I wanted to do was to be able to artifically increase the bit depth of a WAV file. This doesn't get you better resolution note! You still have the original quantisation error from the lower bit depth. It just enabled me to compare it more easily with another WAV file of differing bit depth.
        </p>
        <pre>ffmpeg -acodec pcm_s32le output-file.wav -i input-file.wav</pre>
        <p>
        	The parameter <code>acodec</code> sets the output audio codec to be used.
        </p>

        <h4>Cut out sections</h4>
        <p>
        	To <b>cut out a section</b> of a video and drop audio, use the following:
        </p>
        <pre>ffmpeg -ss 00:00:20 -i [INPUT FILE] -c copy -an -t 00:00:10 [OUTPUT_FILE]
# -ss 00:00:20 - Start 20 seconds in to the video. format is HH:MM:SS
# -c - copy the video i.e don't decode/encode
# -an - drop audio
# -t 00:00:10 - Copy 10 seconds</pre>

		<h4>Compression</h4>
        <p>
            To <b>compress</b> your video or <b>change its size</b> try the following:
        </p>
        <pre>ffmpeg -i &lt;inputfilename&gt; -s 640x480 -b:v 512k &lt;outputfilename&gt;</pre>

        <h4>Duplicate Sources</h4>
        <p>The following copies one V4L2 source to two other V4L2 loopback devices...</p>
        <pre>ffmpeg -hide_banner -i /dev/video0 -c copy -f tee -map 0:v "[f=v4l2]/dev/video10|[f=v4l2]/dev/video11"</pre>

        <h4>formats</h4>
        <p>FFMPEG supports loads of Discover and specify source <a href="https://ffmpeg.org/ffmpeg-formats.html" target="_blank">formats</a>, and can
        additionally tell you what formats your device supports...</p>
        <pre>ffmpeg -f v4l2 -list_formats all -i /dev/videoX</pre>
        <pre>ffmpeg -f v4l2 -input_format mjpeg -video_size=1920x1080 -i /dev/videoX ...</pre>
        <p>To measure the bandwidth taken by a webcam at a desired frame size, for example:</p>
        <pre>ffmpeg -loglevel panic -hide_banner -f v4l2 -input_format rawvideo -video_size 1920x1080 -i /dev/videoX -c:v copy -f rawvideo - | pv > /dev/null</pre>

        <h4>Complex Filtering</h4>
        <pre>https://stackoverflow.com/questions/35341451/how-to-duplicate-a-mpegts-stream-into-multiple-other-udp-streams-using-ffmpeg</pre>
        <pre>https://ffmpeg.org/ffmpeg-filters.html#scale-1 and https://ffmpeg.org/ffmpeg-filters.html#Examples-93</pre>
        <pre>https://trac.ffmpeg.org/wiki/FilteringGuide</pre>


        <h3>ffprobe</h3>
        <p>
            To get information on your video file you can also use <a href="https://ffmpeg.org/ffprobe.html" target="_blank"><code>ffprobe</code></a>. To extract
            the frame information use:
        </p>
        <pre>ffprobe -show_packets -i &lt;your-input-file&gt;</pre>
        <p>
           The <code>ffprobe</code> command lets you select streams too...
        </p>
        <pre># Select only audio streams
ffprobe -show_packets -select_streams a -i &lt;your-input-file&gt;

# Select onl video streams
ffprobe -show_packets -select_streams v -i &lt;your-input-file&gt;

# Select only the video stream with index 1
ffprobe -show_packets -select_streams v:1 -i &lt;your-input-file&gt;</pre>
    	<p>
    		Other useful flags include <code>-show_frames</code>, <code>-show_streams</code>,
    		<code>-show_programs</code>, <code>-count_frames</code>, <code>--count_packets</code>.
    	</p>
        <P>
            For example, to find information of all video streams you could use the following.
            Then by looking for the string &quot;<code>id=0x...</code>&quot; you can find
            the PIDs for each video stream.
        </P>
        <pre>ffprobe -show_streams -select_streams v -i &lt;your-input-file&gt;</pre>
        <p>
            Because the PCR values normally get transmitted with the video stream for any
            program, you could also use the following and grab, from the program dump,
            the string &quot;<code>pcr_pid=xxx&quot;</code>, which should match the
            &quot;<code>id=...</code>&quot; string in the video stream info dump.
        </p>
        <pre>ffprobe -show_programs -select_streams v -i &lt;your-input-file&gt;</pre>


        <h3>CUDA</h3>
        <pre>https://www.nvidia.com/Download/Find.aspx?lang=en-us
https://developer.nvidia.com/ffmpeg
https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1604&target_type=runfilelocal
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</pre>

    </div>

    <h2>V2L2</h2>
    <div>
    	<pre>v4l2-ctl --device=/dev/video0 --info</pre>
	    <pre>v4l2-ctl -d /dev/videoX --list-framesizes=&lt;frame-type&gt;  # Get frame type from --list-formats
             --list-formats[-ext]                  # Use to get frame types
             --(get|set)-input
             --list-inputs
             --set-fmt-video</pre>
	</div>

    <h2>V4L2 Loopback</h2>
    <div>
        <p>
        	If you want to catch an arbitrary video stream and make it appear as a V4L device, i.e., appear as <code>/dev/videoX</code> then
        	you need V4L loopback. It is a kernel module that will create a V4L device that can be fed frame data from any source, which
        	then can be read back out from the V4L device using the V4L API.
        </p>
        <p>
        	To install V4LLoopback:
        </p>
        <pre class="prettyprint linenums">sudo apt install $(uname -r)
sudo apt install libelf-dev

git clone https://github.com/umlaeute/v4l2loopback.git
cd v4l2loopback

make
sudo make install

modprobe v4l2loopback [video_nr=...] [card_label=...]</pre>
		<p>
			As an example, lets say you want to capture some arbitrary transport stream from a multicast channel and make
			that stream appear through a V4L interface:
		</p>
		<pre class="prettyprint linenums">ffmpeg -i udp://239.0.aaa.bbb:port -vcodec rawvideo -pix_fmt yuv420p -threads 0 -f v4l2 /dev/videoX</pre>
		<p>
			You may wish to control the format and size of the V4L device's buffer. In this case you might do something like:
		</p>
		<pre class="prettyprint linenums">sudo v4l2loopback-ctl set-caps "video/x-raw,format=UYVY,width=1280,height=720" /dev/videoX</pre>
		<p>
			If you do this, you must change the transcode of the FFMEG instance feeding this V4L buffer.
		</p>
	</div>

</div> <!-- END H1 padding div -->
</div> <!-- END CONTENT -->

</body>
</html>


 
 
 
 
 
 
 
 
 
 
 
 
